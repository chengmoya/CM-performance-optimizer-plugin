"""
CM æ€§èƒ½ä¼˜åŒ–æ’ä»¶ v4.1.0

åŠŸèƒ½æ¨¡å—ï¼š
1. æ¶ˆæ¯ç¼“å­˜ (message_cache) - ç¼“å­˜ find_messages æŸ¥è¯¢ç»“æœ
2. äººç‰©ä¿¡æ¯ç¼“å­˜ (person_cache) - ç¼“å­˜äººç‰©ä¿¡æ¯æŸ¥è¯¢
3. è¡¨è¾¾å¼ç¼“å­˜ (expression_cache) - åŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢
4. é»‘è¯ç¼“å­˜ (slang_cache) - åŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢+å†…å®¹ç´¢å¼•
5. é¢„åŠ è½½ (preload) - å¼‚æ­¥é¢„åŠ è½½èŠå¤©æµçš„æ¶ˆæ¯å’Œäººç‰©ä¿¡æ¯

å®‰è£…ï¼šå°†ç›®å½•æ”¾å…¥ MaiBot/plugins/ ä¸‹ï¼Œé‡å¯ MaiBot
ä¾èµ–ï¼šæ— é¢å¤–ä¾èµ–
"""

import sys
import asyncio
import time
import threading
from pathlib import Path
from typing import Optional, Dict, Any
from collections import OrderedDict

try:
    from src.plugin_system.apis.plugin_register_api import register_plugin
    from src.plugin_system.base.base_plugin import BasePlugin
    from src.plugin_system.base.config_types import ConfigField, ConfigSection, ConfigLayout, ConfigTab
    from src.common.logger import get_logger
except ImportError:
    class BasePlugin:
        def __init__(self, plugin_dir=None): pass
    class ConfigField:
        def __init__(self, **kw): pass
    class ConfigSection:
        def __init__(self, **kw): pass
    class ConfigLayout:
        def __init__(self, **kw): pass
    class ConfigTab:
        def __init__(self, **kw): pass
    def register_plugin(cls): return cls
    def get_logger(name):
        import logging
        return logging.getLogger(name)

logger = get_logger("CM_perf_opt")


# ===== é€šç”¨ç¼“å­˜ç±» =====
class TTLCache:
    """å¸¦TTLçš„LRUç¼“å­˜"""
    def __init__(self, max_size=500, ttl=120.0):
        self.max_size, self.ttl = max_size, ttl
        self.data = OrderedDict()
        self.ts = {}
        self.lock = threading.Lock()
    
    def get(self, k):
        with self.lock:
            if k not in self.data: return None, False
            if time.time() - self.ts[k] > self.ttl:
                del self.data[k], self.ts[k]
                return None, False
            self.data.move_to_end(k)
            return self.data[k], True
    
    def set(self, k, v):
        with self.lock:
            if len(self.data) >= self.max_size:
                old = next(iter(self.data))
                del self.data[old], self.ts[old]
            self.data[k] = v
            self.ts[k] = time.time()
    
    def invalidate(self, k):
        with self.lock:
            if k in self.data:
                del self.data[k], self.ts[k]
    
    def clear(self):
        with self.lock:
            self.data.clear()
            self.ts.clear()
    
    def size(self): return len(self.data)


# ===== ç»Ÿè®¡ç±» =====
class ModuleStats:
    """å•ä¸ªæ¨¡å—çš„ç»Ÿè®¡"""
    def __init__(self, name: str):
        self.name = name
        self.lock = threading.Lock()
        self.t_hit = self.t_miss = self.t_filtered = 0
        self.i_hit = self.i_miss = self.i_filtered = 0
        self.t_fast = self.t_slow = 0
        self.i_fast = self.i_slow = 0
        self.t_fast_time = self.t_slow_time = 0.0
        self.i_fast_time = self.i_slow_time = 0.0
    
    def hit(self):
        with self.lock:
            self.t_hit += 1
            self.i_hit += 1
    
    def miss(self, elapsed: float):
        with self.lock:
            self.t_miss += 1
            self.i_miss += 1
            if elapsed > 0.1:
                self.t_slow += 1
                self.i_slow += 1
                self.t_slow_time += elapsed
                self.i_slow_time += elapsed
            else:
                self.t_fast += 1
                self.i_fast += 1
                self.t_fast_time += elapsed
                self.i_fast_time += elapsed
    
    def filtered(self):
        """è®°å½•å‘½ä¸­ä½†è¢«è¿‡æ»¤çš„æƒ…å†µï¼ˆå¦‚chat_idä¸åŒ¹é…ï¼‰"""
        with self.lock:
            self.t_filtered += 1
            self.i_filtered += 1
    
    def reset_interval(self) -> Dict[str, Any]:
        with self.lock:
            r = {"i_hit": self.i_hit, "i_miss": self.i_miss, "i_filtered": self.i_filtered,
                 "i_fast": self.i_fast, "i_slow": self.i_slow,
                 "i_fast_time": self.i_fast_time, "i_slow_time": self.i_slow_time}
            self.i_hit = self.i_miss = self.i_filtered = 0
            self.i_fast = self.i_slow = 0
            self.i_fast_time = self.i_slow_time = 0.0
            return r
    
    def total(self) -> Dict[str, Any]:
        with self.lock:
            return {"t_hit": self.t_hit, "t_miss": self.t_miss, "t_filtered": self.t_filtered,
                    "t_fast": self.t_fast, "t_slow": self.t_slow,
                    "t_fast_time": self.t_fast_time, "t_slow_time": self.t_slow_time}


def rate(hit, miss, filtered=0):
    t = hit + miss + filtered
    return (hit / t * 100) if t > 0 else 0


# ===== æ¶ˆæ¯ç¼“å­˜æ¨¡å— =====
class MessageCacheModule:
    """æ¶ˆæ¯æŸ¥è¯¢ç¼“å­˜"""
    def __init__(self, max_size=2000, ttl=120.0):
        self.cache = TTLCache(max_size, ttl)
        self.stats = ModuleStats("message_cache")
        self._orig_func = None
        self._patched = False
    
    def apply_patch(self):
        if self._patched: return
        try:
            from src.common import message_repository
            self._orig_func = message_repository.find_messages
            module = self
            
            def patched(message_filter, sort=None, limit=0, limit_mode="latest",
                       filter_bot=False, filter_command=False, filter_intercept_message_level=None):
                mf = message_filter or {}
                key = f"{mf.get('chat_id','')}:{mf.get('stream_id','')}:{limit}:{limit_mode}:{filter_bot}"
                
                val, hit = module.cache.get(key)
                if hit:
                    module.stats.hit()
                    return val
                
                t0 = time.time()
                res = module._orig_func(message_filter, sort, limit, limit_mode, 
                                        filter_bot, filter_command, filter_intercept_message_level)
                module.stats.miss(time.time() - t0)
                
                if 0 < limit <= 200:
                    module.cache.set(key, res)
                return res
            
            message_repository.find_messages = patched
            # æ›¿æ¢å·²å¯¼å…¥çš„å¼•ç”¨
            for n, m in list(sys.modules.items()):
                if m and getattr(m, 'find_messages', None) is self._orig_func:
                    setattr(m, 'find_messages', patched)
                    logger.debug(f"[MsgCache] æ›¿æ¢ {n}.find_messages")
            
            self._patched = True
            logger.info("[MsgCache] âœ“ è¡¥ä¸åº”ç”¨æˆåŠŸ")
        except Exception as e:
            logger.error(f"[MsgCache] âœ— è¡¥ä¸å¤±è´¥: {e}")
    
    def remove_patch(self):
        if not self._patched or not self._orig_func: return
        try:
            from src.common import message_repository
            message_repository.find_messages = self._orig_func
            self._patched = False
            logger.info("[MsgCache] è¡¥ä¸å·²ç§»é™¤")
        except: pass


# ===== äººç‰©ä¿¡æ¯ç¼“å­˜æ¨¡å— (ä»person-cache-pluginæ•´åˆ) =====
class PersonCacheModule:
    """äººç‰©ä¿¡æ¯ç¼“å­˜"""
    def __init__(self, max_size=3000, ttl=1800):
        self.cache = TTLCache(max_size, ttl)
        self.stats = ModuleStats("person_cache")
        self._orig_load = None
        self._orig_sync = None
        self._patched = False
    
    def apply_patch(self):
        if self._patched: return
        try:
            from src.person_info.person_info import Person
            self._orig_load = Person.load_from_database
            self._orig_sync = Person.sync_to_database
            module = self
            
            def cached_load(self_person):
                person_id = self_person.person_id
                cached = module.cache.get(person_id)
                if cached[1]:  # hit
                    module.stats.hit()
                    for k, v in cached[0].items():
                        setattr(self_person, k, v)
                    return
                
                t0 = time.time()
                module._orig_load(self_person)
                module.stats.miss(time.time() - t0)
                
                if self_person.is_known:
                    data = {
                        "user_id": getattr(self_person, "user_id", ""),
                        "platform": getattr(self_person, "platform", ""),
                        "is_known": getattr(self_person, "is_known", False),
                        "nickname": getattr(self_person, "nickname", ""),
                        "person_name": getattr(self_person, "person_name", None),
                        "name_reason": getattr(self_person, "name_reason", None),
                        "know_times": getattr(self_person, "know_times", 0),
                        "know_since": getattr(self_person, "know_since", None),
                        "last_know": getattr(self_person, "last_know", None),
                        "memory_points": list(getattr(self_person, "memory_points", []) or []),
                        "group_nick_name": list(getattr(self_person, "group_nick_name", []) or []),
                    }
                    module.cache.set(person_id, data)
            
            def cached_sync(self_person):
                module.cache.invalidate(self_person.person_id)
                module._orig_sync(self_person)
            
            Person.load_from_database = cached_load
            Person.sync_to_database = cached_sync
            self._patched = True
            logger.info("[äººç‰©ç¼“å­˜] âœ“ è¡¥ä¸åº”ç”¨æˆåŠŸ")
        except Exception as e:
            logger.error(f"[äººç‰©ç¼“å­˜] âœ— è¡¥ä¸å¤±è´¥: {e}")
    
    def remove_patch(self):
        if not self._patched: return
        try:
            from src.person_info.person_info import Person
            if self._orig_load: Person.load_from_database = self._orig_load
            if self._orig_sync: Person.sync_to_database = self._orig_sync
            self._patched = False
            logger.info("[äººç‰©ç¼“å­˜] è¡¥ä¸å·²ç§»é™¤")
        except: pass


# ===== è¡¨è¾¾å¼ç¼“å­˜æ¨¡å— (åŒç¼“å†² + ç¼“æ…¢åŠ è½½) =====
class ExpressionCacheModule:
    """è¡¨è¾¾å¼å…¨é‡ç¼“å­˜ - åŒç¼“å†² + ç¼“æ…¢åŠ è½½ + åŸå­åˆ‡æ¢"""
    def __init__(self, batch_size=100, batch_delay=0.05, refresh_interval=3600):
        # åŒç¼“å†²
        self.buffer_a = None      # å½“å‰ä½¿ç”¨çš„ç¼“å­˜
        self.buffer_b = None      # åå°åŠ è½½çš„ç¼“å­˜
        self.buffer_lock = threading.Lock()
        
        # åŠ è½½é…ç½®
        self.batch_size = batch_size        # æ¯æ‰¹åŠ è½½æ¡æ•°
        self.batch_delay = batch_delay      # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.refresh_interval = refresh_interval  # è‡ªåŠ¨åˆ·æ–°é—´éš”
        
        # çŠ¶æ€
        self.loading = False        # æ˜¯å¦æ­£åœ¨åŠ è½½
        self.load_lock = asyncio.Lock()
        self.last_refresh = 0       # ä¸Šæ¬¡åˆ·æ–°æ—¶é—´
        self.stats = ModuleStats("expression_cache")
        
        # å¯åŠ¨æ—¶ç«‹å³å¼€å§‹åŠ è½½
        try:
            asyncio.get_running_loop().create_task(self._load_to_buffer_b())
        except RuntimeError:
            pass  # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç¨ååŠ è½½
    
    def get_all(self):
        """è·å–å½“å‰ç¼“å­˜ï¼ˆä»ç¼“å†²åŒºAï¼‰"""
        with self.buffer_lock:
            # å¦‚æœç¼“å†²åŒºAä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆèµ°æ•°æ®åº“ï¼‰
            if self.buffer_a is None:
                return []
            return self.buffer_a
    
    async def _load_to_buffer_b(self):
        """ç¼“æ…¢åŠ è½½æ•°æ®åˆ°ç¼“å†²åŒºB"""
        async with self.load_lock:
            if self.loading:
                return
            self.loading = True
        
        try:
            logger.info("[ExprCache] å¼€å§‹ç¼“æ…¢åŠ è½½è¡¨è¾¾å¼ç¼“å­˜åˆ°ç¼“å†²åŒºB...")
            
            # æ¸…ç©ºç¼“å†²åŒºB
            buffer_b_data = []
            
            # åˆ†æ‰¹åŠ è½½
            offset = 0
            from src.common.database.database_model import Expression
            while True:
                # æŸ¥è¯¢ä¸€æ‰¹æ•°æ®
                batch = list(Expression.select().limit(self.batch_size).offset(offset))
                if not batch:
                    break
                
                # æ·»åŠ åˆ°ç¼“å†²åŒºB
                buffer_b_data.extend(batch)
                
                # è®°å½•è¿›åº¦
                logger.debug(f"[ExprCache] åŠ è½½è¿›åº¦: {len(buffer_b_data)} æ¡")
                
                # ä¼‘çœ ï¼Œé¿å…CPUå³°å€¼
                await asyncio.sleep(self.batch_delay)
                
                offset += self.batch_size
            
            # åŠ è½½å®Œæˆï¼ŒåŸå­åˆ‡æ¢
            with self.buffer_lock:
                self.buffer_b = buffer_b_data
                # åŸå­åˆ‡æ¢ï¼šbuffer_b â†’ buffer_a
                self.buffer_a, self.buffer_b = self.buffer_b, None
                
            self.last_refresh = time.time()
            logger.info(f"[ExprCache] ç¼“å­˜åŠ è½½å®Œæˆå¹¶åˆ‡æ¢: {len(buffer_b_data)} æ¡")
            
        except Exception as e:
            logger.error(f"[ExprCache] ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        finally:
            async with self.load_lock:
                self.loading = False
    
    async def _refresh_loop(self):
        """å®šæœŸåˆ·æ–°å¾ªç¯"""
        while True:
            await asyncio.sleep(self.refresh_interval)
            logger.info("[ExprCache] è§¦å‘å®šæœŸåˆ·æ–°...")
            await self._load_to_buffer_b()
    
    def refresh(self):
        """æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜"""
        try:
            asyncio.get_running_loop().create_task(self._load_to_buffer_b())
            logger.info("[ExprCache] å·²è§¦å‘æ‰‹åŠ¨åˆ·æ–°")
        except RuntimeError:
            logger.warning("[ExprCache] æ— æ³•è§¦å‘åˆ·æ–°ï¼šæ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯")
    
    def size(self):
        """è·å–ç¼“å­˜å¤§å°"""
        with self.buffer_lock:
            return len(self.buffer_a) if self.buffer_a else 0


# ===== é»‘è¯ç¼“å­˜æ¨¡å— (åŒç¼“å†² + ç¼“æ…¢åŠ è½½) =====
class JargonCacheModule:
    """é»‘è¯å…¨é‡ç¼“å­˜ - åŒç¼“å†² + ç¼“æ…¢åŠ è½½ + åŸå­åˆ‡æ¢"""
    def __init__(self, batch_size=100, batch_delay=0.05, refresh_interval=3600, enable_content_index=True):
        # åŒç¼“å†²
        self.buffer_a = None      # å½“å‰ä½¿ç”¨çš„ç¼“å­˜
        self.buffer_b = None      # åå°åŠ è½½çš„ç¼“å­˜
        self.buffer_lock = threading.Lock()
        
        # å†…å®¹ç´¢å¼•
        self.content_index_a = None  # å½“å‰ä½¿ç”¨çš„å†…å®¹ç´¢å¼•
        self.content_index_b = None  # åå°åŠ è½½çš„å†…å®¹ç´¢å¼•
        self.enable_content_index = enable_content_index
        
        # åŠ è½½é…ç½®
        self.batch_size = batch_size        # æ¯æ‰¹åŠ è½½æ¡æ•°
        self.batch_delay = batch_delay      # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.refresh_interval = refresh_interval  # è‡ªåŠ¨åˆ·æ–°é—´éš”
        
        # çŠ¶æ€
        self.loading = False        # æ˜¯å¦æ­£åœ¨åŠ è½½
        self.load_lock = asyncio.Lock()
        self.last_refresh = 0       # ä¸Šæ¬¡åˆ·æ–°æ—¶é—´
        self.stats = ModuleStats("jargon_cache")
        
        # å¯åŠ¨æ—¶ç«‹å³å¼€å§‹åŠ è½½
        try:
            asyncio.get_running_loop().create_task(self._load_to_buffer_b())
        except RuntimeError:
            pass  # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç¨ååŠ è½½
    
    def get_all(self):
        """è·å–å½“å‰ç¼“å­˜ï¼ˆä»ç¼“å†²åŒºAï¼‰"""
        with self.buffer_lock:
            # å¦‚æœç¼“å†²åŒºAä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆèµ°æ•°æ®åº“ï¼‰
            if self.buffer_a is None:
                return []
            return self.buffer_a
    
    def get_by_content(self, content: str):
        """é€šè¿‡å†…å®¹ç²¾ç¡®åŒ¹é…ï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰"""
        if not self.enable_content_index:
            return None
        with self.buffer_lock:
            if self.content_index_a is None:
                return None
            return self.content_index_a.get(content.lower())
    
    async def _load_to_buffer_b(self):
        """ç¼“æ…¢åŠ è½½æ•°æ®åˆ°ç¼“å†²åŒºB"""
        async with self.load_lock:
            if self.loading:
                return
            self.loading = True
        
        try:
            logger.info("[JargonCache] å¼€å§‹ç¼“æ…¢åŠ è½½é»‘è¯ç¼“å­˜åˆ°ç¼“å†²åŒºB...")
            
            # æ¸…ç©ºç¼“å†²åŒºB
            buffer_b_data = []
            content_index_b = {} if self.enable_content_index else None
            
            # åˆ†æ‰¹åŠ è½½
            offset = 0
            from src.common.database.database_model import Jargon
            while True:
                # æŸ¥è¯¢ä¸€æ‰¹æ•°æ®
                batch = list(Jargon.select().limit(self.batch_size).offset(offset))
                if not batch:
                    break
                
                # æ·»åŠ åˆ°ç¼“å†²åŒºB
                buffer_b_data.extend(batch)
                
                # æ„å»ºå†…å®¹ç´¢å¼•
                if self.enable_content_index:
                    for jargon in batch:
                        if jargon.content:
                            content_index_b[jargon.content.lower()] = jargon
                
                # è®°å½•è¿›åº¦
                logger.debug(f"[JargonCache] åŠ è½½è¿›åº¦: {len(buffer_b_data)} æ¡")
                
                # ä¼‘çœ ï¼Œé¿å…CPUå³°å€¼
                await asyncio.sleep(self.batch_delay)
                
                offset += self.batch_size
            
            # åŠ è½½å®Œæˆï¼ŒåŸå­åˆ‡æ¢
            with self.buffer_lock:
                self.buffer_b = buffer_b_data
                self.content_index_b = content_index_b
                # åŸå­åˆ‡æ¢ï¼šbuffer_b â†’ buffer_a
                self.buffer_a, self.buffer_b = self.buffer_b, None
                self.content_index_a, self.content_index_b = self.content_index_b, None
                
            self.last_refresh = time.time()
            logger.info(f"[JargonCache] ç¼“å­˜åŠ è½½å®Œæˆå¹¶åˆ‡æ¢: {len(buffer_b_data)} æ¡")
            
        except Exception as e:
            logger.error(f"[JargonCache] ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        finally:
            async with self.load_lock:
                self.loading = False
    
    async def _refresh_loop(self):
        """å®šæœŸåˆ·æ–°å¾ªç¯"""
        while True:
            await asyncio.sleep(self.refresh_interval)
            logger.info("[JargonCache] è§¦å‘å®šæœŸåˆ·æ–°...")
            await self._load_to_buffer_b()
    
    def refresh(self):
        """æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜"""
        try:
            asyncio.get_running_loop().create_task(self._load_to_buffer_b())
            logger.info("[JargonCache] å·²è§¦å‘æ‰‹åŠ¨åˆ·æ–°")
        except RuntimeError:
            logger.warning("[JargonCache] æ— æ³•è§¦å‘åˆ·æ–°ï¼šæ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯")
    
    def size(self):
        """è·å–ç¼“å­˜å¤§å°"""
        with self.buffer_lock:
            return len(self.buffer_a) if self.buffer_a else 0


# ===== çŸ¥è¯†åº“å›¾è°±ç¼“å­˜æ¨¡å— (åŒç¼“å†² + ç¼“æ…¢åŠ è½½) =====
class KGCacheModule:
    """çŸ¥è¯†åº“å›¾è°±å…¨é‡ç¼“å­˜ - åŒç¼“å†² + ç¼“æ…¢åŠ è½½ + åŸå­åˆ‡æ¢"""
    def __init__(self, batch_size=100, batch_delay=0.05, refresh_interval=3600):
        # åŒç¼“å†²
        self.buffer_a = None      # å½“å‰ä½¿ç”¨çš„å›¾æ•°æ®
        self.buffer_b = None      # åå°åŠ è½½çš„å›¾æ•°æ®
        self.buffer_lock = threading.Lock()
        
        # ç¼“å­˜å†…å®¹
        self.graph_a = None       # å›¾å¯¹è±¡
        self.nodes_a = None       # èŠ‚ç‚¹åˆ—è¡¨
        self.edges_a = None       # è¾¹åˆ—è¡¨
        self.ent_appear_cnt_a = None  # å®ä½“å‡ºç°æ¬¡æ•°
        self.stored_paragraph_hashes_a = None  # æ®µè½hashé›†åˆ
        
        # åŠ è½½é…ç½®
        self.batch_size = batch_size        # æ¯æ‰¹åŠ è½½æ¡æ•°
        self.batch_delay = batch_delay      # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.refresh_interval = refresh_interval  # è‡ªåŠ¨åˆ·æ–°é—´éš”
        
        # çŠ¶æ€
        self.loading = False        # æ˜¯å¦æ­£åœ¨åŠ è½½
        self.load_lock = asyncio.Lock()
        self.last_refresh = 0       # ä¸Šæ¬¡åˆ·æ–°æ—¶é—´
        self.stats = ModuleStats("kg_cache")
        
        # å¯åŠ¨æ—¶ç«‹å³å¼€å§‹åŠ è½½
        try:
            asyncio.get_running_loop().create_task(self._load_to_buffer_b())
        except RuntimeError:
            pass  # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç¨ååŠ è½½
    
    def get_cached_data(self):
        """è·å–å½“å‰ç¼“å­˜çš„å›¾æ•°æ®"""
        with self.buffer_lock:
            if self.buffer_a is None:
                return None
            return {
                "graph": self.graph_a,
                "nodes": self.nodes_a,
                "edges": self.edges_a,
                "ent_appear_cnt": self.ent_appear_cnt_a,
                "stored_paragraph_hashes": self.stored_paragraph_hashes_a,
            }
    
    def is_loaded(self):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å·²åŠ è½½"""
        with self.buffer_lock:
            return self.buffer_a is not None
    
    async def _load_to_buffer_b(self):
        """ç¼“æ…¢åŠ è½½æ•°æ®åˆ°ç¼“å†²åŒºB"""
        async with self.load_lock:
            if self.loading:
                return
            self.loading = True
        
        try:
            logger.info("[KGCache] å¼€å§‹ç¼“æ…¢åŠ è½½çŸ¥è¯†åº“å›¾è°±ç¼“å­˜åˆ°ç¼“å†²åŒºB...")
            
            # å°è¯•åŠ è½½çŸ¥è¯†åº“å›¾è°±
            from src.chat.knowledge.kg_manager import KGManager
            kg_manager = KGManager()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            import os
            if not os.path.exists(kg_manager.graph_data_path):
                logger.warning(f"[KGCache] çŸ¥è¯†åº“å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {kg_manager.graph_data_path}")
                self.loading = False
                return
            
            # åŠ è½½æ•°æ®
            t0 = time.time()
            
            # åŠ è½½å›¾è°±
            from quick_algo import di_graph
            graph_b = di_graph.load_from_file(kg_manager.graph_data_path)
            nodes_b = graph_b.get_node_list()
            edges_b = graph_b.get_edge_list()
            
            logger.debug(f"[KGCache] åŠ è½½å›¾è°±: {len(nodes_b)} ä¸ªèŠ‚ç‚¹, {len(edges_b)} æ¡è¾¹")
            
            # åŠ è½½å®ä½“è®¡æ•°
            import pandas as pd
            ent_cnt_df = pd.read_parquet(kg_manager.ent_cnt_data_path, engine="pyarrow")
            ent_appear_cnt_b = dict({row["hash_key"]: row["appear_cnt"] for _, row in ent_cnt_df.iterrows()})
            
            logger.debug(f"[KGCache] åŠ è½½å®ä½“è®¡æ•°: {len(ent_appear_cnt_b)} ä¸ªå®ä½“")
            
            # åŠ è½½æ®µè½hash
            import json
            with open(kg_manager.pg_hash_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                stored_paragraph_hashes_b = set(data["stored_paragraph_hashes"])
            
            logger.debug(f"[KGCache] åŠ è½½æ®µè½hash: {len(stored_paragraph_hashes_b)} ä¸ªæ®µè½")
            
            # æ¨¡æ‹Ÿåˆ†æ‰¹åŠ è½½çš„å»¶è¿Ÿï¼ˆé¿å…CPUå³°å€¼ï¼‰
            total_items = len(nodes_b) + len(edges_b) + len(ent_appear_cnt_b)
            batches = max(1, (total_items + self.batch_size - 1) // self.batch_size)
            for i in range(batches):
                await asyncio.sleep(self.batch_delay)
                if i % 10 == 0:
                    logger.debug(f"[KGCache] åŠ è½½è¿›åº¦: {i+1}/{batches} æ‰¹")
            
            # åŠ è½½å®Œæˆï¼ŒåŸå­åˆ‡æ¢
            with self.buffer_lock:
                self.buffer_b = True
                self.graph_b = graph_b
                self.nodes_b = nodes_b
                self.edges_b = edges_b
                self.ent_appear_cnt_b = ent_appear_cnt_b
                self.stored_paragraph_hashes_b = stored_paragraph_hashes_b
                # åŸå­åˆ‡æ¢ï¼šbuffer_b â†’ buffer_a
                self.buffer_a, self.buffer_b = self.buffer_b, None
                self.graph_a, self.graph_b = self.graph_b, None
                self.nodes_a, self.nodes_b = self.nodes_b, None
                self.edges_a, self.edges_b = self.edges_b, None
                self.ent_appear_cnt_a, self.ent_appear_cnt_b = self.ent_appear_cnt_b, None
                self.stored_paragraph_hashes_a, self.stored_paragraph_hashes_b = self.stored_paragraph_hashes_b, None
                
            self.last_refresh = time.time()
            load_time = time.time() - t0
            logger.info(f"[KGCache] ç¼“å­˜åŠ è½½å®Œæˆå¹¶åˆ‡æ¢: èŠ‚ç‚¹{len(nodes_b)}ä¸ª, è¾¹{len(edges_b)}æ¡, è€—æ—¶{load_time:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"[KGCache] ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        finally:
            async with self.load_lock:
                self.loading = False
    
    async def _refresh_loop(self):
        """å®šæœŸåˆ·æ–°å¾ªç¯"""
        while True:
            await asyncio.sleep(self.refresh_interval)
            logger.info("[KGCache] è§¦å‘å®šæœŸåˆ·æ–°...")
            await self._load_to_buffer_b()
    
    def refresh(self):
        """æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜"""
        try:
            asyncio.get_running_loop().create_task(self._load_to_buffer_b())
            logger.info("[KGCache] å·²è§¦å‘æ‰‹åŠ¨åˆ·æ–°")
        except RuntimeError:
            logger.warning("[KGCache] æ— æ³•è§¦å‘åˆ·æ–°ï¼šæ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯")
    
    def size(self):
        """è·å–ç¼“å­˜å¤§å°"""
        with self.buffer_lock:
            if self.nodes_a is None:
                return 0
            return {
                "nodes": len(self.nodes_a),
                "edges": len(self.edges_a),
                "entities": len(self.ent_appear_cnt_a),
                "paragraphs": len(self.stored_paragraph_hashes_a),
            }


# ===== é¢„åŠ è½½ç®¡ç†å™¨ =====
class PreloadManager:
    """é¢„åŠ è½½ç®¡ç†å™¨ - ç®¡ç†èŠå¤©æµçš„é¢„åŠ è½½çŠ¶æ€"""
    def __init__(self, max_streams=10):
        self.preloaded_streams = set()  # å·²é¢„åŠ è½½çš„èŠå¤©æµID
        self.max_streams = max_streams
        self.stream_last_active = {}  # èŠå¤©æµæœ€åæ´»è·ƒæ—¶é—´
        self.lock = threading.Lock()
    
    def should_preload(self, stream_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„åŠ è½½"""
        with self.lock:
            if stream_id in self.preloaded_streams:
                # æ›´æ–°æ´»è·ƒæ—¶é—´
                self.stream_last_active[stream_id] = time.time()
                return False
            return True
    
    def mark_preloaded(self, stream_id: str):
        """æ ‡è®°ä¸ºå·²é¢„åŠ è½½"""
        with self.lock:
            self.preloaded_streams.add(stream_id)
            self.stream_last_active[stream_id] = time.time()
            
            # å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼Œç§»é™¤æœ€æ—§çš„
            if len(self.preloaded_streams) > self.max_streams:
                oldest = min(self.stream_last_active.items(), key=lambda x: x[1])[0]
                self.preloaded_streams.remove(oldest)
                del self.stream_last_active[oldest]
    
    def get_stats(self):
        """è·å–é¢„åŠ è½½ç»Ÿè®¡"""
        with self.lock:
            return {
                "preloaded_count": len(self.preloaded_streams),
                "max_streams": self.max_streams,
                "streams": list(self.preloaded_streams)
            }


# ===== é¢„åŠ è½½äº‹ä»¶å¤„ç†å™¨ =====
class PreloadEventHandler:
    """é¢„åŠ è½½äº‹ä»¶å¤„ç†å™¨ - ç›‘å¬æ¶ˆæ¯äº‹ä»¶å¹¶é¢„åŠ è½½èŠå¤©æµæ•°æ®"""
    def __init__(self, preload_manager, message_count=50, max_persons_per_stream=50, preload_delay=0.1):
        self.preload_manager = preload_manager
        self.message_count = message_count
        self.max_persons_per_stream = max_persons_per_stream
        self.preload_delay = preload_delay
        self.enabled = True
    
    async def handle_message(self, message):
        """å¤„ç†æ¶ˆæ¯äº‹ä»¶"""
        if not self.enabled:
            return True, True, None, None, None
        
        if not message or not hasattr(message, 'stream_id') or not message.stream_id:
            return True, True, None, None, None
        
        stream_id = message.stream_id
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„åŠ è½½
        if not self.preload_manager.should_preload(stream_id):
            return True, True, None, None, None
        
        # å¼‚æ­¥é¢„åŠ è½½
        asyncio.create_task(self._preload_stream_data(stream_id))
        
        return True, True, None, None, None
    
    async def _preload_stream_data(self, stream_id: str):
        """é¢„åŠ è½½èŠå¤©æµæ•°æ®"""
        try:
            # å»¶è¿Ÿæ‰§è¡Œï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
            await asyncio.sleep(self.preload_delay)
            
            # 1. åŠ è½½æœ€è¿‘çš„æ¶ˆæ¯ï¼ˆè¿™ä¼šè‡ªåŠ¨å¡«å……æ¶ˆæ¯ç¼“å­˜ï¼‰
            from src.common import message_repository
            messages = message_repository.find_messages(
                {"stream_id": stream_id},
                limit=self.message_count,
                limit_mode="latest"
            )
            
            # 2. æå–æ¶ˆæ¯ä¸­çš„å‘é€è€…IDå’Œå¹³å°ç»„åˆ
            person_keys = set()
            for msg in messages:
                if hasattr(msg, "user_id") and msg.user_id and hasattr(msg, "user_platform") and msg.user_platform:
                    person_keys.add((msg.user_platform, msg.user_id))
            
            # 3. é¢„åŠ è½½è¿™äº›äººç‰©ä¿¡æ¯
            from src.person_info.person_info import Person
            loaded_count = 0
            for platform, user_id in list(person_keys)[:self.max_persons_per_stream]:
                person_id = f"{platform}_{user_id}"
                person = Person(person_id=person_id)
                person.load_from_database()  # è¿™ä¼šè‡ªåŠ¨å¡«å……äººç‰©ç¼“å­˜
                loaded_count += 1
            
            # 4. æ ‡è®°ä¸ºå·²é¢„åŠ è½½
            self.preload_manager.mark_preloaded(stream_id)
            
            logger.info(f"[Preload] é¢„åŠ è½½å®Œæˆ: {stream_id[:20]}..., æ¶ˆæ¯{len(messages)}æ¡, äººç‰©{loaded_count}ä¸ª")
        except Exception as e:
            logger.error(f"[Preload] é¢„åŠ è½½å¤±è´¥: {e}")


# ===== ä¸»ä¼˜åŒ–å™¨ =====
class Optimizer:
    _inst = None
    
    def __new__(cls, *a, **kw):
        if not cls._inst:
            cls._inst = super().__new__(cls)
            cls._inst._ready = False
        return cls._inst
    
    def __init__(self, cfg=None):
        if self._ready: return
        cfg = cfg or {}
        self.start_time = time.time()
        self.interval = cfg.get("report_interval", 60)
        self.modules_cfg = cfg.get("modules", {})
        
        # åˆå§‹åŒ–æ¨¡å—
        self.msg_cache = None
        self.person_cache = None
        self.expr_cache = None
        self.jargon_cache = None
        self.kg_cache = None
        
        if self.modules_cfg.get("message_cache", True):
            self.msg_cache = MessageCacheModule(
                cfg.get("message_cache_size", 2000),
                cfg.get("message_cache_ttl", 120.0)
            )
        
        if self.modules_cfg.get("person_cache", True):
            self.person_cache = PersonCacheModule(
                cfg.get("person_cache_size", 3000),
                cfg.get("person_cache_ttl", 1800)
            )
        
        if self.modules_cfg.get("expression_cache", False):
            self.expr_cache = ExpressionCacheModule(
                batch_size=cfg.get("expression_cache_batch_size", 100),
                batch_delay=cfg.get("expression_cache_batch_delay", 0.05),
                refresh_interval=cfg.get("expression_cache_refresh_interval", 3600)
            )
        
        if self.modules_cfg.get("slang_cache", False):
            self.jargon_cache = JargonCacheModule(
                batch_size=cfg.get("slang_cache_batch_size", 100),
                batch_delay=cfg.get("slang_cache_batch_delay", 0.05),
                refresh_interval=cfg.get("slang_cache_refresh_interval", 3600),
                enable_content_index=cfg.get("slang_cache_enable_content_index", True)
            )
        
        if self.modules_cfg.get("kg_cache", False):
            self.kg_cache = KGCacheModule(
                batch_size=cfg.get("kg_cache_batch_size", 100),
                batch_delay=cfg.get("kg_cache_batch_delay", 0.05),
                refresh_interval=cfg.get("kg_cache_refresh_interval", 3600)
            )
        
        # åˆå§‹åŒ–é¢„åŠ è½½åŠŸèƒ½
        self.preload_manager = None
        self.preload_handler = None
        if self.modules_cfg.get("preload_enabled", False):
            self.preload_manager = PreloadManager(
                max_streams=cfg.get("preload_max_streams", 10)
            )
            self.preload_handler = PreloadEventHandler(
                preload_manager=self.preload_manager,
                message_count=cfg.get("preload_message_count", 50),
                max_persons_per_stream=cfg.get("preload_max_persons_per_stream", 50),
                preload_delay=cfg.get("preload_delay", 0.1)
            )
        
        self._running = False
        self._ready = True
    
    def apply_patches(self):
        if self.msg_cache:
            self.msg_cache.apply_patch()
        if self.person_cache:
            self.person_cache.apply_patch()
        
        # è¡¨è¾¾å¼ç¼“å­˜æ‹¦æˆª
        if self.expr_cache:
            self._apply_expression_cache_patch()
        
        # é»‘è¯ç¼“å­˜æ‹¦æˆª
        if self.jargon_cache:
            self._apply_jargon_cache_patch()
        
        # çŸ¥è¯†åº“å›¾è°±ç¼“å­˜æ‹¦æˆª
        if self.kg_cache:
            self._apply_kg_cache_patch()
    
    def _apply_expression_cache_patch(self):
        """åº”ç”¨è¡¨è¾¾å¼ç¼“å­˜æ‹¦æˆª"""
        try:
            from src.bw_learner.expression_learner import ExpressionLearner
            orig_find_similar = ExpressionLearner._find_similar_situation_expression
            expr_cache = self.expr_cache
            stats = self.expr_cache.stats
            
            async def patched_find_similar(learner_self, situation: str, similarity_threshold: float = 0.75):
                # ä»ç¼“å­˜è·å–æ‰€æœ‰è¡¨è¾¾å¼
                all_expressions = expr_cache.get_all()
                
                # å¦‚æœç¼“å­˜æœªåŠ è½½ï¼Œèµ°åŸé€»è¾‘
                if not all_expressions:
                    logger.debug("[ExprCache] ç¼“å­˜æœªåŠ è½½ï¼Œä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢")
                    t0 = time.time()
                    result = await orig_find_similar(learner_self, situation, similarity_threshold)
                    stats.miss(time.time() - t0)
                    logger.debug(f"[ExprCache] ç¼“å­˜æœªå‘½ä¸­(æœªåŠ è½½): è€—æ—¶={time.time()-t0:.3f}s")
                    return result
                
                # åœ¨ç¼“å­˜ä¸­è¿‡æ»¤å½“å‰ chat_id çš„è¡¨è¾¾å¼
                chat_expressions = [expr for expr in all_expressions if expr.chat_id == learner_self.chat_id]
                
                # å…ˆåœ¨æ‰€æœ‰è¡¨è¾¾å¼ä¸­æŸ¥æ‰¾åŒ¹é…ï¼ˆç”¨äºç»Ÿè®¡è¢«è¿‡æ»¤çš„æƒ…å†µï¼‰
                best_match_all = None
                best_similarity_all = 0.0
                matched_chat_id_all = None
                
                for expr in all_expressions:
                    content_list = learner_self._parse_content_list(expr.content_list)
                    for existing_situation in content_list:
                        from src.bw_learner.learner_utils import calculate_similarity
                        similarity = calculate_similarity(situation, existing_situation)
                        if similarity >= similarity_threshold and similarity > best_similarity_all:
                            best_similarity_all = similarity
                            best_match_all = expr
                            matched_chat_id_all = expr.chat_id
                
                # åœ¨å½“å‰ chat_id çš„è¡¨è¾¾å¼ä¸­æŸ¥æ‰¾åŒ¹é…
                best_match = None
                best_similarity = 0.0
                
                for expr in chat_expressions:
                    content_list = learner_self._parse_content_list(expr.content_list)
                    for existing_situation in content_list:
                        from src.bw_learner.learner_utils import calculate_similarity
                        similarity = calculate_similarity(situation, existing_situation)
                        if similarity >= similarity_threshold and similarity > best_similarity:
                            best_similarity = similarity
                            best_match = expr
                
                if best_match:
                    stats.hit()
                    logger.debug(f"[ExprCache] ç¼“å­˜å‘½ä¸­: ç›¸ä¼¼åº¦={best_similarity:.3f}, ç°æœ‰='{best_match.situation}', æ–°='{situation}'")
                elif best_match_all:
                    # åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°åŒ¹é…ï¼Œä½† chat_id ä¸åŒ¹é…
                    stats.filtered()
                    logger.debug(f"[ExprCache] ç¼“å­˜å‘½ä¸­ä½†è¢«è¿‡æ»¤: situation='{situation}', åŒ¹é…chat_id={matched_chat_id_all}, æŸ¥è¯¢chat_id={learner_self.chat_id}, ç›¸ä¼¼åº¦={best_similarity_all:.3f}")
                else:
                    stats.miss(0.0)  # ç¼“å­˜ä¸­æœªæ‰¾åˆ°ï¼Œä½†æŸ¥è¯¢å¾ˆå¿«
                    logger.debug(f"[ExprCache] ç¼“å­˜æœªå‘½ä¸­(æ— åŒ¹é…): situation='{situation}'")
                
                return best_match, best_similarity
            
            ExpressionLearner._find_similar_situation_expression = patched_find_similar
            logger.info("[ExprCache] âœ“ è¡¨è¾¾å¼ç¼“å­˜æ‹¦æˆªå·²åº”ç”¨")
        except Exception as e:
            logger.error(f"[ExprCache] âœ— è¡¨è¾¾å¼ç¼“å­˜æ‹¦æˆªå¤±è´¥: {e}")
    
    def _apply_jargon_cache_patch(self):
        """åº”ç”¨é»‘è¯ç¼“å­˜æ‹¦æˆª"""
        try:
            from src.bw_learner.jargon_explainer import JargonExplainer
            from src.bw_learner.learner_utils import is_bot_message, contains_bot_self_name, parse_chat_id_list, chat_id_list_contains
            from src.config.config import global_config
            import re
            
            orig_match_jargon = JargonExplainer.match_jargon_from_messages
            jargon_cache = self.jargon_cache
            stats = self.jargon_cache.stats
            
            def patched_match_jargon(explainer_self, messages):
                # ä»ç¼“å­˜è·å–æ‰€æœ‰é»‘è¯
                all_jargons = jargon_cache.get_all()
                
                # å¦‚æœç¼“å­˜æœªåŠ è½½ï¼Œèµ°åŸé€»è¾‘
                if not all_jargons:
                    logger.debug("[JargonCache] ç¼“å­˜æœªåŠ è½½ï¼Œä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢")
                    t0 = time.time()
                    result = orig_match_jargon(explainer_self, messages)
                    stats.miss(time.time() - t0)
                    logger.debug(f"[JargonCache] ç¼“å­˜æœªå‘½ä¸­(æœªåŠ è½½): è€—æ—¶={time.time()-t0:.3f}s, æ¶ˆæ¯æ•°={len(messages)}")
                    return result
                
                # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹ï¼ˆè·³è¿‡æœºå™¨äººæ¶ˆæ¯ï¼‰
                message_texts = []
                for msg in messages:
                    if is_bot_message(msg):
                        continue
                    
                    msg_text = (
                        getattr(msg, "display_message", None) or
                        getattr(msg, "processed_plain_text", None) or ""
                    ).strip()
                    if msg_text:
                        message_texts.append(msg_text)
                
                if not message_texts:
                    stats.miss(0.0)
                    logger.debug("[JargonCache] ç¼“å­˜æœªå‘½ä¸­(æ— æœ‰æ•ˆæ¶ˆæ¯)")
                    return []
                
                # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯æ–‡æœ¬
                combined_text = " ".join(message_texts)
                
                # æ ¹æ® all_global_jargon é…ç½®å†³å®šæŸ¥è¯¢é€»è¾‘
                all_global_jargon = global_config.expression.all_global_jargon
                
                # åœ¨ç¼“å­˜ä¸­è¿‡æ»¤æœ‰meaningçš„é»‘è¯
                valid_jargons = [j for j in all_jargons if j.meaning and j.meaning.strip()]
                
                # ç”¨äºç»Ÿè®¡è¢«è¿‡æ»¤çš„åŒ¹é…
                filtered_matches = []
                
                # åœ¨åˆå¹¶æ–‡æœ¬ä¸­æŸ¥æ‰¾åŒ¹é…
                matched_jargon = {}
                hit_count = 0
                
                for jargon in valid_jargons:
                    content = jargon.content or ""
                    if not content or not content.strip():
                        continue
                    
                    # è·³è¿‡åŒ…å«æœºå™¨äººæ˜µç§°çš„è¯æ¡
                    if contains_bot_self_name(content):
                        continue
                    
                    # æ£€æŸ¥chat_idï¼ˆå¦‚æœall_global=Falseï¼‰
                    if not all_global_jargon:
                        if jargon.is_global:
                            # å…¨å±€é»‘è¯ï¼ŒåŒ…å«
                            pass
                        else:
                            # æ£€æŸ¥chat_idåˆ—è¡¨æ˜¯å¦åŒ…å«å½“å‰chat_id
                            chat_id_list = parse_chat_id_list(jargon.chat_id)
                            if not chat_id_list_contains(chat_id_list, explainer_self.chat_id):
                                # è®°å½•è¢«è¿‡æ»¤çš„åŒ¹é…ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
                                # æ£€æŸ¥æ˜¯å¦åœ¨æ–‡æœ¬ä¸­åŒ¹é…
                                pattern = re.escape(content)
                                if re.search(r"[\u4e00-\u9fff]", content):
                                    search_pattern = pattern
                                else:
                                    search_pattern = r"\b" + pattern + r"\b"
                                
                                if re.search(search_pattern, combined_text, re.IGNORECASE):
                                    filtered_matches.append((content, jargon.chat_id))
                                continue
                    
                    # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾åŒ¹é…ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
                    pattern = re.escape(content)
                    # ä½¿ç”¨å•è¯è¾¹ç•Œæˆ–ä¸­æ–‡å­—ç¬¦è¾¹ç•Œæ¥åŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                    if re.search(r"[\u4e00-\u9fff]", content):
                        # åŒ…å«ä¸­æ–‡ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…
                        search_pattern = pattern
                    else:
                        # çº¯è‹±æ–‡/æ•°å­—ï¼Œä½¿ç”¨å•è¯è¾¹ç•Œ
                        search_pattern = r"\b" + pattern + r"\b"
                    
                    if re.search(search_pattern, combined_text, re.IGNORECASE):
                        # æ‰¾åˆ°åŒ¹é…ï¼Œè®°å½•ï¼ˆå»é‡ï¼‰
                        if content not in matched_jargon:
                            matched_jargon[content] = {"content": content}
                            hit_count += 1
                
                # ç»Ÿè®¡å‘½ä¸­/æœªå‘½ä¸­/è¢«è¿‡æ»¤
                if hit_count > 0:
                    stats.hit()
                    logger.debug(f"[JargonCache] ç¼“å­˜å‘½ä¸­: åŒ¹é…åˆ° {hit_count} ä¸ªé»‘è¯: {list(matched_jargon.keys())}")
                elif filtered_matches:
                    stats.filtered()
                    filtered_sample = filtered_matches[:3]  # åªæ˜¾ç¤ºå‰3ä¸ª
                    logger.debug(f"[JargonCache] ç¼“å­˜å‘½ä¸­ä½†è¢«è¿‡æ»¤: åŒ¹é…åˆ° {len(filtered_matches)} ä¸ªé»‘è¯ä½†chat_idä¸åŒ¹é…ï¼Œç¤ºä¾‹: {filtered_sample}")
                else:
                    stats.miss(0.0)
                    logger.debug(f"[JargonCache] ç¼“å­˜æœªå‘½ä¸­(æ— åŒ¹é…): æ¶ˆæ¯æ•°={len(messages)}, æœ‰æ•ˆé»‘è¯æ•°={len(valid_jargons)}, æ–‡æœ¬é•¿åº¦={len(combined_text)}")
                
                return list(matched_jargon.values())
            
            JargonExplainer.match_jargon_from_messages = patched_match_jargon
            logger.info("[JargonCache] âœ“ é»‘è¯ç¼“å­˜æ‹¦æˆªå·²åº”ç”¨")
        except Exception as e:
            logger.error(f"[JargonCache] âœ— é»‘è¯ç¼“å­˜æ‹¦æˆªå¤±è´¥: {e}")
    
    def _apply_kg_cache_patch(self):
        """åº”ç”¨çŸ¥è¯†åº“å›¾è°±ç¼“å­˜æ‹¦æˆª"""
        try:
            from src.chat.knowledge.kg_manager import KGManager
            
            orig_load_from_file = KGManager.load_from_file
            kg_cache = self.kg_cache
            stats = self.kg_cache.stats
            
            def patched_load_from_file(self_kg):
                # ä»ç¼“å­˜è·å–å›¾æ•°æ®
                cached_data = kg_cache.get_cached_data()
                
                # å¦‚æœç¼“å­˜æœªåŠ è½½ï¼Œèµ°åŸé€»è¾‘
                if cached_data is None:
                    logger.debug("[KGCache] ç¼“å­˜æœªåŠ è½½ï¼Œä½¿ç”¨æ–‡ä»¶åŠ è½½")
                    t0 = time.time()
                    result = orig_load_from_file(self_kg)
                    stats.miss(time.time() - t0)
                    logger.debug(f"[KGCache] ç¼“å­˜æœªå‘½ä¸­(æœªåŠ è½½): è€—æ—¶={time.time()-t0:.3f}s")
                    return result
                
                # ä½¿ç”¨ç¼“å­˜æ•°æ®
                t0 = time.time()
                
                # ç›´æ¥èµ‹å€¼ç¼“å­˜çš„æ•°æ®
                self_kg.graph = cached_data["graph"]
                self_kg.ent_appear_cnt = cached_data["ent_appear_cnt"]
                self_kg.stored_paragraph_hashes = cached_data["stored_paragraph_hashes"]
                
                stats.hit()
                elapsed = time.time() - t0
                logger.debug(f"[KGCache] ç¼“å­˜å‘½ä¸­: è€—æ—¶={elapsed:.3f}s, èŠ‚ç‚¹æ•°={len(cached_data['nodes'])}, è¾¹æ•°={len(cached_data['edges'])}")
                
                return
            
            KGManager.load_from_file = patched_load_from_file
            logger.info("[KGCache] âœ“ çŸ¥è¯†åº“å›¾è°±ç¼“å­˜æ‹¦æˆªå·²åº”ç”¨")
        except Exception as e:
            logger.error(f"[KGCache] âœ— çŸ¥è¯†åº“å›¾è°±ç¼“å­˜æ‹¦æˆªå¤±è´¥: {e}")
    
    async def _report_loop(self):
        logger.info(f"[PerfOpt] ç»Ÿè®¡æŠ¥å‘Šå¯åŠ¨ (é—´éš”{self.interval}s)")
        while self._running:
            await asyncio.sleep(self.interval)
            if not self._running: break
            self._print_report()
    
    def _print_report(self):
        uptime = int(time.time() - self.start_time)
        uptime_str = f"{uptime//3600}h{(uptime%3600)//60}m{uptime%60}s"
        
        # æ„å»ºå®Œæ•´çš„æŠ¥å‘Šå†…å®¹
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append(f"ğŸ“Š CMæ€§èƒ½ä¼˜åŒ–æ’ä»¶ç»Ÿè®¡æŠ¥å‘Š | è¿è¡Œæ—¶é—´: {uptime_str}")
        report_lines.append("=" * 80)
        
        # æ¶ˆæ¯ç¼“å­˜
        if self.msg_cache:
            report_lines.extend(self._build_module_stats_lines("ğŸ“¦ æ¶ˆæ¯ç¼“å­˜", self.msg_cache))
            report_lines.append("")
        
        # äººç‰©ç¼“å­˜
        if self.person_cache:
            report_lines.extend(self._build_module_stats_lines("ğŸ‘¤ äººç‰©ç¼“å­˜", self.person_cache))
            report_lines.append("")
        
        # è¡¨è¾¾å¼ç¼“å­˜
        if self.expr_cache:
            report_lines.extend(self._build_full_cache_stats_lines("ğŸ­ è¡¨è¾¾å¼ç¼“å­˜", self.expr_cache))
            report_lines.append("")
        
        # é»‘è¯ç¼“å­˜
        if self.jargon_cache:
            report_lines.extend(self._build_full_cache_stats_lines("ğŸ—£ï¸ é»‘è¯ç¼“å­˜", self.jargon_cache))
            report_lines.append("")
        
        # çŸ¥è¯†åº“å›¾è°±ç¼“å­˜
        if self.kg_cache:
            report_lines.extend(self._build_kg_cache_stats_lines("ğŸ§  çŸ¥è¯†åº“å›¾è°±ç¼“å­˜", self.kg_cache))
            report_lines.append("")
        
        report_lines.append("=" * 80)
        
        # ä¸€æ¬¡æ€§æ‰“å°æ‰€æœ‰è¡Œï¼Œå‡å°‘æ—¥å¿—ç³»ç»Ÿå¼€é”€
        logger.info("\n".join(report_lines))
    
    def _build_full_cache_stats_lines(self, name: str, module):
        """æ„å»ºå…¨é‡ç¼“å­˜ç»Ÿè®¡çš„è¡Œ"""
        lines = []
        size = module.size()
        loading_status = "åŠ è½½ä¸­" if module.loading else "å·²åŠ è½½"
        last_refresh = time.time() - module.last_refresh if module.last_refresh > 0 else 0
        last_refresh_str = f"{int(last_refresh//60)}m{int(last_refresh%60)}så‰" if last_refresh > 0 else "ä»æœª"
        
        # æ˜¾ç¤ºå‘½ä¸­ç»Ÿè®¡
        t = module.stats.total()
        i = module.stats.reset_interval()
        t_total = t["t_hit"] + t["t_miss"] + t["t_filtered"]
        i_total = i["i_hit"] + i["i_miss"] + i["i_filtered"]
        t_rate = rate(t["t_hit"], t_total) if t_total > 0 else 0
        i_rate = rate(i["i_hit"], i_total) if i_total > 0 else 0
        t_time = t["t_fast_time"] + t["t_slow_time"]
        i_time = i["i_fast_time"] + i["i_slow_time"]
        
        # ä¼°ç®—èŠ‚çœæ—¶é—´
        avg_time = t_time / t["t_miss"] if t["t_miss"] > 0 else 0.02
        saved = t["t_hit"] * avg_time
        
        lines.append(f"{name}")
        lines.append(f"  çŠ¶æ€: {loading_status} | å¤§å°: {size}æ¡ | ä¸Šæ¬¡åˆ·æ–°: {last_refresh_str}")
        if module.refresh_interval > 0:
            lines.append(f"  è‡ªåŠ¨åˆ·æ–°: æ¯{module.refresh_interval}ç§’")
        lines.append(f"  ç´¯è®¡: å‘½ä¸­ {t['t_hit']} | æœªå‘½ä¸­ {t['t_miss']} | è¢«è¿‡æ»¤ {t['t_filtered']} | å‘½ä¸­ç‡ {t_rate:.1f}%")
        lines.append(f"  æœ¬æœŸ: å‘½ä¸­ {i['i_hit']} | æœªå‘½ä¸­ {i['i_miss']} | è¢«è¿‡æ»¤ {i['i_filtered']} | å‘½ä¸­ç‡ {i_rate:.1f}%")
        lines.append(f"  èŠ‚çœ: {saved:.1f}ç§’ (å¹³å‡{avg_time*1000:.1f}ms/æ¬¡)")
        
        return lines
    
    def _build_module_stats_lines(self, name: str, module):
        """æ„å»ºæ¨¡å—ç»Ÿè®¡çš„è¡Œ"""
        lines = []
        t = module.stats.total()
        i = module.stats.reset_interval()
        t_total = t["t_hit"] + t["t_miss"] + t["t_filtered"]
        i_total = i["i_hit"] + i["i_miss"] + i["i_filtered"]
        t_rate = rate(t["t_hit"], t_total) if t_total > 0 else 0
        i_rate = rate(i["i_hit"], i_total) if i_total > 0 else 0
        t_time = t["t_fast_time"] + t["t_slow_time"]
        i_time = i["i_fast_time"] + i["i_slow_time"]
        
        # ä¼°ç®—èŠ‚çœæ—¶é—´
        avg_time = t_time / t["t_miss"] if t["t_miss"] > 0 else 0.03
        saved = t["t_hit"] * avg_time
        
        lines.append(f"{name}")
        lines.append(f"  ç¼“å­˜: {module.cache.size()}/{module.cache.max_size} | TTL: {module.cache.ttl}ç§’")
        lines.append(f"  ç´¯è®¡: å‘½ä¸­ {t['t_hit']} | æœªå‘½ä¸­ {t['t_miss']} | è¢«è¿‡æ»¤ {t['t_filtered']} | å‘½ä¸­ç‡ {t_rate:.1f}%")
        lines.append(f"  ç´¯è®¡: å¿« {t['t_fast']}æ¬¡/{t['t_fast_time']:.2f}s | æ…¢ {t['t_slow']}æ¬¡/{t['t_slow_time']:.2f}s")
        lines.append(f"  æœ¬æœŸ: å‘½ä¸­ {i['i_hit']} | æœªå‘½ä¸­ {i['i_miss']} | è¢«è¿‡æ»¤ {i['i_filtered']} | å‘½ä¸­ç‡ {i_rate:.1f}%")
        lines.append(f"  èŠ‚çœ: {saved:.1f}ç§’ (å¹³å‡{avg_time*1000:.1f}ms/æ¬¡)")
        
        return lines
    
    def _build_kg_cache_stats_lines(self, name: str, module):
        """æ„å»ºçŸ¥è¯†åº“å›¾è°±ç¼“å­˜ç»Ÿè®¡çš„è¡Œ"""
        lines = []
        size = module.size()
        if size == 0:
            size_str = "æœªåŠ è½½"
        else:
            size_str = f"èŠ‚ç‚¹{size['nodes']}ä¸ª, è¾¹{size['edges']}æ¡, å®ä½“{size['entities']}ä¸ª, æ®µè½{size['paragraphs']}ä¸ª"
        
        loading_status = "åŠ è½½ä¸­" if module.loading else "å·²åŠ è½½"
        last_refresh = time.time() - module.last_refresh if module.last_refresh > 0 else 0
        last_refresh_str = f"{int(last_refresh//60)}m{int(last_refresh%60)}så‰" if last_refresh > 0 else "ä»æœª"
        
        # æ˜¾ç¤ºå‘½ä¸­ç»Ÿè®¡
        t = module.stats.total()
        i = module.stats.reset_interval()
        t_total = t["t_hit"] + t["t_miss"] + t["t_filtered"]
        i_total = i["i_hit"] + i["i_miss"] + i["i_filtered"]
        t_rate = rate(t["t_hit"], t_total) if t_total > 0 else 0
        i_rate = rate(i["i_hit"], i_total) if i_total > 0 else 0
        t_time = t["t_fast_time"] + t["t_slow_time"]
        i_time = i["i_fast_time"] + i["i_slow_time"]
        
        # ä¼°ç®—èŠ‚çœæ—¶é—´
        avg_time = t_time / t["t_miss"] if t["t_miss"] > 0 else 0.5
        saved = t["t_hit"] * avg_time
        
        lines.append(f"{name}")
        lines.append(f"  çŠ¶æ€: {loading_status} | å¤§å°: {size_str} | ä¸Šæ¬¡åˆ·æ–°: {last_refresh_str}")
        if module.refresh_interval > 0:
            lines.append(f"  è‡ªåŠ¨åˆ·æ–°: æ¯{module.refresh_interval}ç§’")
        lines.append(f"  ç´¯è®¡: å‘½ä¸­ {t['t_hit']} | æœªå‘½ä¸­ {t['t_miss']} | è¢«è¿‡æ»¤ {t['t_filtered']} | å‘½ä¸­ç‡ {t_rate:.1f}%")
        lines.append(f"  æœ¬æœŸ: å‘½ä¸­ {i['i_hit']} | æœªå‘½ä¸­ {i['i_miss']} | è¢«è¿‡æ»¤ {i['i_filtered']} | å‘½ä¸­ç‡ {i_rate:.1f}%")
        lines.append(f"  èŠ‚çœ: {saved:.1f}ç§’ (å¹³å‡{avg_time*1000:.1f}ms/æ¬¡)")
        
        return lines
    
    def start(self):
        if self._running: return
        self._running = True
        try:
            asyncio.get_running_loop().create_task(self._report_loop())
            # å¯åŠ¨è¡¨è¾¾å¼å’Œé»‘è¯ç¼“å­˜çš„å®šæœŸåˆ·æ–°
            if self.expr_cache and self.expr_cache.refresh_interval > 0:
                asyncio.get_running_loop().create_task(self.expr_cache._refresh_loop())
            if self.jargon_cache and self.jargon_cache.refresh_interval > 0:
                asyncio.get_running_loop().create_task(self.jargon_cache._refresh_loop())
            if self.kg_cache and self.kg_cache.refresh_interval > 0:
                asyncio.get_running_loop().create_task(self.kg_cache._refresh_loop())
        except: pass
    
    def stop(self):
        self._running = False
        if self.msg_cache: self.msg_cache.remove_patch()
        if self.person_cache: self.person_cache.remove_patch()


_opt: Optional[Optimizer] = None

config_fields = {
    # ===== æ’ä»¶åŸºæœ¬é…ç½® (ç¬¬1ä¸ªæ ‡ç­¾é¡µ) =====
    "plugin": {
        "enabled": ConfigField(type=bool, default=True, description="æ˜¯å¦å¯ç”¨æ’ä»¶"),
        "version": ConfigField(type=str, default="4.1.0", description="æ’ä»¶ç‰ˆæœ¬å·ï¼Œç”¨äºè¿½è¸ªæ›´æ–°"),
        "report_interval": ConfigField(type=int, default=60, description="ç»Ÿè®¡æŠ¥å‘Šè¾“å‡ºé—´éš”(ç§’)ï¼Œè®¾ç½®0å¯å…³é—­å®šæ—¶æŠ¥å‘Š", min=0, max=600),
        "log_level": ConfigField(type=str, default="INFO", description="æ—¥å¿—è¾“å‡ºç­‰çº§", choices=["DEBUG", "INFO", "WARNING", "ERROR"]),
    },
    # ===== æ¨¡å—å¼€å…³ (ç¬¬2ä¸ªæ ‡ç­¾é¡µ) =====
    "modules": {
        "message_cache_enabled": ConfigField(type=bool, default=True, description="æ¶ˆæ¯ç¼“å­˜: æ‹¦æˆªfind_messagesæ•°æ®åº“æŸ¥è¯¢ï¼Œç¼“å­˜ç»“æœé¿å…é‡å¤æŸ¥è¯¢ã€‚å‘½ä¸­ç‡é€šå¸¸>95%ï¼Œå¯èŠ‚çœå¤§é‡æ•°æ®åº“IO"),
        "person_cache_enabled": ConfigField(type=bool, default=True, description="äººç‰©ä¿¡æ¯ç¼“å­˜: æ‹¦æˆªäººç‰©ä¿¡æ¯åŠ è½½ï¼ŒæŒ‰QQå·ç¼“å­˜æ˜µç§°ç­‰ä¿¡æ¯ã€‚äººç‰©ä¿¡æ¯å˜åŒ–æ…¢ï¼Œç¼“å­˜æ•ˆæœå¥½"),
        "expression_cache_enabled": ConfigField(type=bool, default=False, description="è¡¨è¾¾å¼ç¼“å­˜: åŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢ï¼Œå…¨é‡ç¼“å­˜è¡¨è¾¾å¼æ•°æ®ã€‚å¯åŠ¨åçº¦10ç§’å®ŒæˆåŠ è½½"),
        "slang_cache_enabled": ConfigField(type=bool, default=False, description="é»‘è¯ç¼“å­˜: åŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢+å†…å®¹ç´¢å¼•ï¼ŒO(1)æŸ¥æ‰¾é€Ÿåº¦ã€‚å¯åŠ¨åçº¦10ç§’å®ŒæˆåŠ è½½"),
        "kg_cache_enabled": ConfigField(type=bool, default=False, description="çŸ¥è¯†åº“å›¾è°±ç¼“å­˜: åŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢ï¼Œå…¨é‡ç¼“å­˜çŸ¥è¯†åº“å›¾è°±æ•°æ®ã€‚å¯åŠ¨åçº¦5-10ç§’å®ŒæˆåŠ è½½ï¼ŒæŸ¥è¯¢é€Ÿåº¦æå‡80-90%"),
        "preload_enabled": ConfigField(type=bool, default=False, description="é¢„åŠ è½½åŠŸèƒ½: å½“èŠå¤©æµæ¿€æ´»æ—¶ï¼Œå¼‚æ­¥é¢„åŠ è½½è¯¥èŠå¤©æµçš„æœ€è¿‘æ¶ˆæ¯å’Œäººç‰©ä¿¡æ¯ã€‚å¯æå‡é¦–æ¬¡æŸ¥è¯¢æ€§èƒ½90%ä»¥ä¸Š"),
    },
    # ===== æ¶ˆæ¯ç¼“å­˜é…ç½® (ç¬¬3ä¸ªæ ‡ç­¾é¡µ) =====
    "message_cache": {
        "max_size": ConfigField(type=int, default=2000, description="æœ€å¤§ç¼“å­˜æ¡ç›®æ•°ã€‚æ¯æ¡çº¦å ç”¨1-5KBå†…å­˜ï¼Œ2000æ¡çº¦å ç”¨2-10MBã€‚è¶…è¿‡åè‡ªåŠ¨æ¸…ç†æœ€æ—§çš„æ¡ç›®", min=100, max=10000),
        "ttl": ConfigField(type=float, default=120.0, description="ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)ã€‚æ¶ˆæ¯å˜åŒ–å¿«ï¼Œå»ºè®®60-180ç§’ã€‚è¿‡é•¿å¯èƒ½å¯¼è‡´æ¶ˆæ¯ä¸åŒæ­¥", min=10.0, max=600.0),
    },
    # ===== äººç‰©ä¿¡æ¯ç¼“å­˜é…ç½® (ç¬¬4ä¸ªæ ‡ç­¾é¡µ) =====
    "person_cache": {
        "max_size": ConfigField(type=int, default=3000, description="æœ€å¤§ç¼“å­˜æ¡ç›®æ•°ã€‚æ¯æ¡çº¦å ç”¨0.5-2KBå†…å­˜ï¼Œ3000æ¡çº¦å ç”¨1.5-6MBã€‚å»ºè®®å¤§äºæ´»è·ƒç”¨æˆ·æ•°", min=100, max=10000),
        "ttl": ConfigField(type=int, default=1800, description="ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)ã€‚äººç‰©ä¿¡æ¯å˜åŒ–æ…¢ï¼Œå»ºè®®1800ç§’(30åˆ†é’Ÿ)ã€‚è¿‡æœŸåè‡ªåŠ¨åˆ·æ–°", min=60, max=7200),
    },
    # ===== è¡¨è¾¾å¼ç¼“å­˜é…ç½® (ç¬¬5ä¸ªæ ‡ç­¾é¡µ) =====
    "expression_cache": {
        "batch_size": ConfigField(type=int, default=100, description="æ¯æ‰¹åŠ è½½çš„æ¡æ•°ã€‚é»˜è®¤100æ¡ï¼Œ2ä¸‡æ¡çº¦éœ€10ç§’åŠ è½½å®Œæˆã€‚å¢å¤§æ­¤å€¼å¯åŠ å¿«åŠ è½½ä½†ä¼šå¢åŠ CPUå³°å€¼", min=10, max=1000),
        "batch_delay": ConfigField(type=str, default="0.05", description="æ‰¹æ¬¡é—´å»¶è¿Ÿ(ç§’)ã€‚ç”¨äºå¹³æ»‘åŠ è½½é¿å…CPUå³°å€¼ï¼Œå¢å¤§æ­¤å€¼å¯é™ä½CPUå ç”¨ä½†å»¶é•¿åŠ è½½æ—¶é—´", choices=["0.01", "0.02", "0.05", "0.1", "0.2", "0.5", "1.0"]),
        "refresh_interval": ConfigField(type=int, default=3600, description="è‡ªåŠ¨åˆ·æ–°é—´éš”(ç§’)ã€‚è®¾ç½®ä¸º0è¡¨ç¤ºä¸è‡ªåŠ¨åˆ·æ–°ï¼Œä»…å¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡ã€‚å»ºè®®3600ç§’(1å°æ—¶)", min=0, max=86400),
    },
    # ===== é»‘è¯ç¼“å­˜é…ç½® (ç¬¬6ä¸ªæ ‡ç­¾é¡µ) =====
    "slang_cache": {
        "batch_size": ConfigField(type=int, default=100, description="æ¯æ‰¹åŠ è½½çš„æ¡æ•°ã€‚é»˜è®¤100æ¡ï¼Œ2ä¸‡æ¡çº¦éœ€10ç§’åŠ è½½å®Œæˆã€‚å¢å¤§æ­¤å€¼å¯åŠ å¿«åŠ è½½ä½†ä¼šå¢åŠ CPUå³°å€¼", min=10, max=1000),
        "batch_delay": ConfigField(type=str, default="0.05", description="æ‰¹æ¬¡é—´å»¶è¿Ÿ(ç§’)ã€‚ç”¨äºå¹³æ»‘åŠ è½½é¿å…CPUå³°å€¼ï¼Œå¢å¤§æ­¤å€¼å¯é™ä½CPUå ç”¨ä½†å»¶é•¿åŠ è½½æ—¶é—´", choices=["0.01", "0.02", "0.05", "0.1", "0.2", "0.5", "1.0"]),
        "refresh_interval": ConfigField(type=int, default=3600, description="è‡ªåŠ¨åˆ·æ–°é—´éš”(ç§’)ã€‚è®¾ç½®ä¸º0è¡¨ç¤ºä¸è‡ªåŠ¨åˆ·æ–°ï¼Œä»…å¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡ã€‚å»ºè®®3600ç§’(1å°æ—¶)", min=0, max=86400),
        "enable_content_index": ConfigField(type=bool, default=True, description="å¯ç”¨å†…å®¹ç´¢å¼•ã€‚å¯ç”¨åå¯é€šè¿‡å†…å®¹å¿«é€ŸæŸ¥æ‰¾é»‘è¯ï¼ŒO(1)æŸ¥æ‰¾é€Ÿåº¦ã€‚ä¼šé¢å¤–å ç”¨å†…å­˜ï¼Œæ¯æ¡çº¦0.1KB"),
    },
    # ===== çŸ¥è¯†åº“å›¾è°±ç¼“å­˜é…ç½® (ç¬¬7ä¸ªæ ‡ç­¾é¡µ) =====
    "kg_cache": {
        "batch_size": ConfigField(type=int, default=100, description="æ¯æ‰¹åŠ è½½çš„æ¡æ•°ã€‚é»˜è®¤100æ¡ï¼Œ2ä¸‡æ¡çº¦éœ€10ç§’åŠ è½½å®Œæˆã€‚å¢å¤§æ­¤å€¼å¯åŠ å¿«åŠ è½½ä½†ä¼šå¢åŠ CPUå³°å€¼", min=10, max=1000),
        "batch_delay": ConfigField(type=str, default="0.05", description="æ‰¹æ¬¡é—´å»¶è¿Ÿ(ç§’)ã€‚ç”¨äºå¹³æ»‘åŠ è½½é¿å…CPUå³°å€¼ï¼Œå¢å¤§æ­¤å€¼å¯é™ä½CPUå ç”¨ä½†å»¶é•¿åŠ è½½æ—¶é—´", choices=["0.01", "0.02", "0.05", "0.1", "0.2", "0.5", "1.0"]),
        "refresh_interval": ConfigField(type=int, default=3600, description="è‡ªåŠ¨åˆ·æ–°é—´éš”(ç§’)ã€‚è®¾ç½®ä¸º0è¡¨ç¤ºä¸è‡ªåŠ¨åˆ·æ–°ï¼Œä»…å¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡ã€‚å»ºè®®3600ç§’(1å°æ—¶)", min=0, max=86400),
    },
    # ===== é¢„åŠ è½½é…ç½® (ç¬¬8ä¸ªæ ‡ç­¾é¡µ) =====
    "preload": {
        "max_streams": ConfigField(type=int, default=10, description="æœ€å¤šé¢„åŠ è½½çš„èŠå¤©æµæ•°é‡ã€‚è¶…è¿‡æ­¤æ•°é‡ä¼šç§»é™¤æœ€æ—§çš„èŠå¤©æµã€‚å»ºè®®10-20ä¸ªï¼Œè¿‡å¤šä¼šå¢åŠ å†…å­˜å ç”¨", min=1, max=50),
        "message_count": ConfigField(type=int, default=50, description="æ¯ä¸ªèŠå¤©æµé¢„åŠ è½½çš„æ¶ˆæ¯æ•°é‡ã€‚å»ºè®®50-100æ¡ï¼Œè¦†ç›–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡", min=10, max=200),
        "max_persons_per_stream": ConfigField(type=int, default=50, description="æ¯ä¸ªèŠå¤©æµé¢„åŠ è½½çš„æœ€å¤§äººç‰©æ•°é‡ã€‚ç¾¤èŠå»ºè®®50-100ä¸ªï¼Œç§èŠ2ä¸ªå³å¯", min=1, max=200),
        "preload_delay": ConfigField(type=str, default="0.1", description="é¢„åŠ è½½å»¶è¿Ÿ(ç§’)ã€‚æ”¶åˆ°æ¶ˆæ¯åå»¶è¿Ÿæ‰§è¡Œé¢„åŠ è½½ï¼Œé¿å…é˜»å¡ä¸»æµç¨‹ã€‚å»ºè®®0.1-0.5ç§’", choices=["0.05", "0.1", "0.2", "0.5", "1.0"]),
    },
}

# é…ç½®èŠ‚æè¿°
config_section_descriptions = {
    "plugin": ConfigSection(
        title="æ’ä»¶è®¾ç½®",
        description="åŸºç¡€é…ç½®ï¼šå¯ç”¨/ç¦ç”¨ã€ç»Ÿè®¡æŠ¥å‘Šé—´éš”ã€æ—¥å¿—ç­‰çº§ã€‚å†…å­˜å ç”¨çº¦10-20MBï¼ŒCPUå¼€é”€æä½",
        icon="ğŸ”§",
        collapsed=False,
        order=0
    ),
    "modules": ConfigSection(
        title="åŠŸèƒ½æ¨¡å—",
        description="é€‰æ‹©å¯ç”¨çš„ç¼“å­˜æ¨¡å—ã€‚æ¶ˆæ¯ç¼“å­˜å‘½ä¸­ç‡é€šå¸¸>95%ï¼Œäººç‰©ä¿¡æ¯ç¼“å­˜å‘½ä¸­ç‡>90%ã€‚å¯æ ¹æ®éœ€è¦å•ç‹¬å¼€å…³",
        icon="ğŸ“¦",
        collapsed=False,
        order=1
    ),
    "message_cache": ConfigSection(
        title="æ¶ˆæ¯ç¼“å­˜",
        description="ç¼“å­˜æ¶ˆæ¯æŸ¥è¯¢ç»“æœã€‚åŸç†ï¼šæ‹¦æˆªæ•°æ®åº“æŸ¥è¯¢ï¼Œç›¸åŒå‚æ•°ç›´æ¥è¿”å›ç¼“å­˜ã€‚æ•ˆæœï¼šå‡å°‘çº¦95%çš„æ•°æ®åº“æŸ¥è¯¢",
        icon="ğŸ’¬",
        collapsed=True,
        order=2
    ),
    "person_cache": ConfigSection(
        title="äººç‰©ä¿¡æ¯ç¼“å­˜",
        description="ç¼“å­˜äººç‰©ä¿¡æ¯(æ˜µç§°ã€å¤‡æ³¨ç­‰)ã€‚åŸç†ï¼šæŒ‰QQå·ç¼“å­˜ï¼Œé¿å…é‡å¤æŸ¥è¯¢æ•°æ®åº“ã€‚æ•ˆæœï¼šå‡å°‘çº¦90%çš„äººç‰©ä¿¡æ¯æŸ¥è¯¢",
        icon="ğŸ‘¤",
        collapsed=True,
        order=3
    ),
    "expression_cache": ConfigSection(
        title="è¡¨è¾¾å¼ç¼“å­˜",
        description="å…¨é‡ç¼“å­˜è¡¨è¾¾å¼æ•°æ®ã€‚åŸç†ï¼šåŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢ï¼Œé¿å…CPUå³°å€¼ã€‚æ•ˆæœï¼šå¯åŠ¨åçº¦10ç§’å®ŒæˆåŠ è½½ï¼Œåç»­æŸ¥è¯¢ç›´æ¥ä»å†…å­˜è¯»å–",
        icon="ğŸ­",
        collapsed=True,
        order=4
    ),
    "slang_cache": ConfigSection(
        title="é»‘è¯ç¼“å­˜",
        description="å…¨é‡ç¼“å­˜é»‘è¯/ç½‘ç»œç”¨è¯­æ•°æ®ã€‚åŸç†ï¼šåŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢ï¼Œæ”¯æŒå†…å®¹ç´¢å¼•O(1)æŸ¥æ‰¾ã€‚æ•ˆæœï¼šå¯åŠ¨åçº¦10ç§’å®ŒæˆåŠ è½½ï¼Œé»‘è¯åŒ¹é…é€Ÿåº¦æå‡100å€ä»¥ä¸Š",
        icon="ğŸ—£ï¸",
        collapsed=True,
        order=5
    ),
    "kg_cache": ConfigSection(
        title="çŸ¥è¯†åº“å›¾è°±ç¼“å­˜",
        description="å…¨é‡ç¼“å­˜çŸ¥è¯†åº“å›¾è°±æ•°æ®ã€‚åŸç†ï¼šåŒç¼“å†²+ç¼“æ…¢åŠ è½½+åŸå­åˆ‡æ¢ï¼Œé¿å…CPUå³°å€¼ã€‚æ•ˆæœï¼šå¯åŠ¨åçº¦5-10ç§’å®ŒæˆåŠ è½½ï¼ŒçŸ¥è¯†åº“æŸ¥è¯¢é€Ÿåº¦æå‡80-90%ï¼Œæ¶ˆé™¤æ–‡ä»¶IOå¼€é”€",
        icon="ğŸ§ ",
        collapsed=True,
        order=6
    ),
    "preload": ConfigSection(
        title="é¢„åŠ è½½",
        description="é¢„åŠ è½½èŠå¤©æµæ•°æ®ã€‚åŸç†ï¼šå½“èŠå¤©æµæ¿€æ´»æ—¶ï¼Œå¼‚æ­¥é¢„åŠ è½½æœ€è¿‘æ¶ˆæ¯å’Œäººç‰©ä¿¡æ¯åˆ°ç¼“å­˜ã€‚æ•ˆæœï¼šé¦–æ¬¡æŸ¥è¯¢æ€§èƒ½æå‡90%ä»¥ä¸Šï¼Œå‡å°‘å†·å¯åŠ¨å»¶è¿Ÿ",
        icon="âš¡",
        collapsed=True,
        order=7
    ),
}

# å¸ƒå±€é…ç½® - ä½¿ç”¨æ ‡ç­¾é¡µå¸ƒå±€
config_layout = ConfigLayout(
    type="tabs",
    tabs=[
        ConfigTab(id="plugin", title="æ’ä»¶", icon="ğŸ”§", sections=["plugin"], order=0),
        ConfigTab(id="modules", title="æ¨¡å—å¼€å…³", icon="ğŸ“¦", sections=["modules"], order=1),
        ConfigTab(id="message_cache", title="æ¶ˆæ¯ç¼“å­˜", icon="ğŸ’¬", sections=["message_cache"], order=2),
        ConfigTab(id="person_cache", title="äººç‰©ä¿¡æ¯ç¼“å­˜", icon="ğŸ‘¤", sections=["person_cache"], order=3),
        ConfigTab(id="expression_cache", title="è¡¨è¾¾å¼ç¼“å­˜", icon="ğŸ­", sections=["expression_cache"], order=4),
        ConfigTab(id="slang_cache", title="é»‘è¯ç¼“å­˜", icon="ğŸ—£ï¸", sections=["slang_cache"], order=5),
        ConfigTab(id="kg_cache", title="çŸ¥è¯†åº“å›¾è°±ç¼“å­˜", icon="ğŸ§ ", sections=["kg_cache"], order=6),
        ConfigTab(id="preload", title="é¢„åŠ è½½", icon="âš¡", sections=["preload"], order=7),
    ]
)


@register_plugin
class PerformanceOptimizerPlugin(BasePlugin):
    plugin_name = "CM-performance-optimizer"
    plugin_version = "4.2.0"
    plugin_description = "æ€§èƒ½ä¼˜åŒ– - æ¶ˆæ¯ç¼“å­˜ + äººç‰©ä¿¡æ¯ç¼“å­˜ + è¡¨è¾¾å¼ç¼“å­˜ + é»‘è¯ç¼“å­˜ + çŸ¥è¯†åº“å›¾è°±ç¼“å­˜ + é¢„åŠ è½½"
    plugin_author = "åŸé™Œ"
    enable_plugin = True
    config_file_name = "config.toml"
    dependencies = []
    python_dependencies = []
    config_schema = config_fields
    config_section_descriptions = config_section_descriptions
    config_layout = config_layout
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        global _opt
        logger.info("[PerfOpt] CM-performance-optimizer v4.2.0 å¯åŠ¨")
        
        try:
            cfg = {
                "report_interval": 60,
                "modules": {"message_cache": True, "person_cache": True, "expression_cache": False, "slang_cache": False, "kg_cache": False, "preload_enabled": False},
                "message_cache_size": 2000, "message_cache_ttl": 120.0,
                "person_cache_size": 3000, "person_cache_ttl": 1800,
                "expression_cache_batch_size": 100, "expression_cache_batch_delay": 0.05, "expression_cache_refresh_interval": 3600,
                "slang_cache_batch_size": 100, "slang_cache_batch_delay": 0.05, "slang_cache_refresh_interval": 3600, "slang_cache_enable_content_index": True,
                "kg_cache_batch_size": 100, "kg_cache_batch_delay": 0.05, "kg_cache_refresh_interval": 3600,
                "preload_max_streams": 10, "preload_message_count": 50, "preload_max_persons_per_stream": 50, "preload_delay": 0.1,
            }
            enabled = True
            log_level = "INFO"
            
            try:
                import tomlkit
                p = Path(__file__).parent / "config.toml"
                if p.exists():
                    with open(p) as f: c = tomlkit.load(f)
                    # plugin æ ‡ç­¾é¡µ
                    enabled = c.get("plugin", {}).get("enabled", True)
                    cfg["report_interval"] = c.get("plugin", {}).get("report_interval", 60)
                    log_level = c.get("plugin", {}).get("log_level", "INFO")
                    # modules æ ‡ç­¾é¡µ
                    modules = c.get("modules", {})
                    cfg["modules"]["message_cache"] = modules.get("message_cache_enabled", True)
                    cfg["modules"]["person_cache"] = modules.get("person_cache_enabled", True)
                    cfg["modules"]["expression_cache"] = modules.get("expression_cache_enabled", False)
                    cfg["modules"]["slang_cache"] = modules.get("slang_cache_enabled", False)
                    cfg["modules"]["kg_cache"] = modules.get("kg_cache_enabled", False)
                    cfg["modules"]["preload_enabled"] = modules.get("preload_enabled", False)
                    # message_cache æ ‡ç­¾é¡µ
                    cfg["message_cache_size"] = c.get("message_cache", {}).get("max_size", 2000)
                    cfg["message_cache_ttl"] = c.get("message_cache", {}).get("ttl", 120.0)
                    # person_cache æ ‡ç­¾é¡µ
                    cfg["person_cache_size"] = c.get("person_cache", {}).get("max_size", 3000)
                    cfg["person_cache_ttl"] = c.get("person_cache", {}).get("ttl", 1800)
                    # expression_cache æ ‡ç­¾é¡µ
                    cfg["expression_cache_batch_size"] = c.get("expression_cache", {}).get("batch_size", 100)
                    # batch_delay ä»å­—ç¬¦ä¸²è½¬æ¢ä¸º float
                    expr_batch_delay_str = c.get("expression_cache", {}).get("batch_delay", "0.05")
                    try:
                        cfg["expression_cache_batch_delay"] = float(expr_batch_delay_str)
                    except (ValueError, TypeError):
                        cfg["expression_cache_batch_delay"] = 0.05
                    cfg["expression_cache_refresh_interval"] = c.get("expression_cache", {}).get("refresh_interval", 3600)
                    # slang_cache æ ‡ç­¾é¡µ
                    cfg["slang_cache_batch_size"] = c.get("slang_cache", {}).get("batch_size", 100)
                    # batch_delay ä»å­—ç¬¦ä¸²è½¬æ¢ä¸º float
                    slang_batch_delay_str = c.get("slang_cache", {}).get("batch_delay", "0.05")
                    try:
                        cfg["slang_cache_batch_delay"] = float(slang_batch_delay_str)
                    except (ValueError, TypeError):
                        cfg["slang_cache_batch_delay"] = 0.05
                    cfg["slang_cache_refresh_interval"] = c.get("slang_cache", {}).get("refresh_interval", 3600)
                    cfg["slang_cache_enable_content_index"] = c.get("slang_cache", {}).get("enable_content_index", True)
                    # kg_cache æ ‡ç­¾é¡µ
                    cfg["kg_cache_batch_size"] = c.get("kg_cache", {}).get("batch_size", 100)
                    # batch_delay ä»å­—ç¬¦ä¸²è½¬æ¢ä¸º float
                    kg_batch_delay_str = c.get("kg_cache", {}).get("batch_delay", "0.05")
                    try:
                        cfg["kg_cache_batch_delay"] = float(kg_batch_delay_str)
                    except (ValueError, TypeError):
                        cfg["kg_cache_batch_delay"] = 0.05
                    cfg["kg_cache_refresh_interval"] = c.get("kg_cache", {}).get("refresh_interval", 3600)
                    # preload æ ‡ç­¾é¡µ
                    cfg["preload_max_streams"] = c.get("preload", {}).get("max_streams", 10)
                    cfg["preload_message_count"] = c.get("preload", {}).get("message_count", 50)
                    cfg["preload_max_persons_per_stream"] = c.get("preload", {}).get("max_persons_per_stream", 50)
                    # preload_delay ä»å­—ç¬¦ä¸²è½¬æ¢ä¸º float
                    preload_delay_str = c.get("preload", {}).get("preload_delay", "0.1")
                    try:
                        cfg["preload_delay"] = float(preload_delay_str)
                    except (ValueError, TypeError):
                        cfg["preload_delay"] = 0.1
            except: pass
            
            # åº”ç”¨æ—¥å¿—ç­‰çº§
            import logging
            level_map = {"DEBUG": logging.DEBUG, "INFO": logging.INFO, "WARNING": logging.WARNING, "ERROR": logging.ERROR}
            if log_level.upper() in level_map:
                logger.setLevel(level_map[log_level.upper()])
                logger.info(f"[PerfOpt] æ—¥å¿—ç­‰çº§: {log_level.upper()}")
            
            if not enabled:
                logger.info("[PerfOpt] æ’ä»¶å·²ç¦ç”¨")
                return
            
            _opt = Optimizer(cfg)
            _opt.apply_patches()
            _opt.start()
            logger.info("[PerfOpt] âœ“ æ’ä»¶å¯åŠ¨å®Œæˆ")
        except Exception as e:
            logger.error(f"[PerfOpt] å¯åŠ¨å¤±è´¥: {e}")
    
    def get_plugin_components(self): return []